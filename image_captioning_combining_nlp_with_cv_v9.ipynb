{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MiRx69k8KcCJ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "This metric implements the mean Fréchet GTE Distance (FGD) score for text embeddings using the GTE-small model.\n",
        "The metric measures the similarity between ground truth and predicted text captions by comparing their embedding distributions.\n",
        "\n",
        "The score is calculated by:\n",
        "1. Converting text to embeddings using GTE-small\n",
        "2. Computing mean and covariance statistics of the embeddings\n",
        "3. Calculating the FGD score between the distributions\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import pandas.api.types\n",
        "import numpy as np\n",
        "from numpy import cov, trace, iscomplexobj\n",
        "from scipy.linalg import sqrtm\n",
        "from typing import List\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def calculate_fgd(solution_embed: np.ndarray, submission_embed: np.ndarray) -> float:\n",
        "    '''\n",
        "    solution_embed: Embedding of the ground truth from GTE-small.\n",
        "    submission_embed: Embedding of the predicted caption from GTE-small.\n",
        "    '''\n",
        "    fgd_list = []\n",
        "    for _idx, (sol_emb_sample, sub_emb_sample) in enumerate(zip(solution_embed, submission_embed)):\n",
        "        sol_emb_sample_rshaped, sub_emb_sample_rshaped = sol_emb_sample.reshape((1,384)), sub_emb_sample.reshape((1,384))\n",
        "        e1 = np.concatenate([sol_emb_sample_rshaped, sol_emb_sample_rshaped])\n",
        "        e2 = np.concatenate([sub_emb_sample_rshaped, sub_emb_sample_rshaped])\n",
        "        \"\"\"Calculate Fréchet GTE Distance between two embedding distributions\"\"\"\n",
        "        # Calculate mean and covariance statistics\n",
        "        mu1, sigma1 = e1.mean(axis=0), cov(e1, rowvar=False)\n",
        "        mu2, sigma2 = e2.mean(axis=0), cov(e2, rowvar=False)\n",
        "\n",
        "        # Calculate sum squared difference between means\n",
        "        ssdiff = np.sum((mu1 - mu2)**2.0)\n",
        "\n",
        "        # Calculate sqrt of product between cov\n",
        "        covmean = sqrtm(sigma1.dot(sigma2))\n",
        "\n",
        "        # Check and correct imaginary numbers from sqrt\n",
        "        if iscomplexobj(covmean):\n",
        "            covmean = covmean.real\n",
        "\n",
        "        # Calculate score\n",
        "        fgd = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "        fgd_list.append(fgd)\n",
        "        if _idx % 100 == 0:\n",
        "            print(f\"Processed {_idx} samples\", end=\"\\r\")\n",
        "    return float(np.mean(fgd_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kLB0STlFKpi9"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Optimized Imports\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW, lr_scheduler\n",
        "from torchvision import transforms\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Memory optimization\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.set_float32_matmul_precision('medium')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRH24pWKKqQo",
        "outputId": "7076196a-3005-4677-cc3f-1b083f5784ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Train shape: (21367, 2), Test shape: (3771, 1)\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Enhanced Data Loading with Streaming\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!test -d \"/content/dataset\" || unzip -q \"/content/drive/MyDrive/caption_dataset/caption_dataset.zip\" -d \"/content/dataset/\"\n",
        "\n",
        "DATASET_PATH = \"/content/dataset/\"\n",
        "TRAIN_IMAGE_DIR = os.path.join(DATASET_PATH, \"train\", \"train\")\n",
        "TEST_IMAGE_DIR = os.path.join(DATASET_PATH, \"test\", \"test\")\n",
        "TRAIN_CSV = os.path.join(DATASET_PATH, \"train.csv\")\n",
        "TEST_CSV = os.path.join(DATASET_PATH, \"test.csv\")\n",
        "\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "test_df = pd.read_csv(TEST_CSV)\n",
        "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8M2vV7q4KqHk"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Corrected Data Pipeline\n",
        "class OptimizedDataset(Dataset):\n",
        "    def __init__(self, df, image_dir, processor):\n",
        "        self.df = df\n",
        "        self.image_dir = image_dir\n",
        "        self.processor = processor\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((384, 384)),  # BLIP's native size\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.df.iloc[idx]['image_id']\n",
        "        caption = self.df.iloc[idx]['caption']\n",
        "        img_path = os.path.join(self.image_dir, f\"{img_id}.jpg\")\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            image = self.transform(image)\n",
        "        except:\n",
        "            image = torch.zeros(3, 384, 384)\n",
        "\n",
        "        inputs = self.processor(\n",
        "            images=image,\n",
        "            text=caption,\n",
        "            return_tensors=\"pt\",\n",
        "            padding='max_length',\n",
        "            max_length=32,\n",
        "            truncation=True,\n",
        "            do_rescale=False  # Critical fix for image scaling\n",
        "        )\n",
        "        inputs['labels'] = inputs['input_ids'].clone()  # Essential for loss calculation\n",
        "        return {k: v.squeeze(0) for k, v in inputs.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiUCEGcjKqDO"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Optimized Model Setup\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "\n",
        "# Freeze entire model first\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Correctly access decoder layers\n",
        "decoder_layers = model.text_decoder.bert.encoder.layer  # Updated path\n",
        "\n",
        "# Unfreeze last 2 decoder layers\n",
        "for layer in decoder_layers[-2:]:\n",
        "    for param in layer.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Unfreeze cross-attention layers\n",
        "for param in model.text_decoder.cls.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "model = model.to('cuda', memory_format=torch.channels_last)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ibEadUYmKxcP"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Streaming DataLoader\n",
        "def chunked_loader(df, chunk_size=5000):\n",
        "    for i in range(0, len(df), chunk_size):\n",
        "        chunk = df.iloc[i:i+chunk_size]\n",
        "        dataset = OptimizedDataset(chunk, TRAIN_IMAGE_DIR, processor)\n",
        "        yield DataLoader(\n",
        "            dataset,\n",
        "            batch_size=8,\n",
        "            shuffle=True,\n",
        "            pin_memory=True,\n",
        "            num_workers=2\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFWEKJxOKzmt",
        "outputId": "04694930-dcb6-466f-c465-5947b446ee52"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-68b656ac3427>:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()  # Original syntax for current versions\n",
            "100%|██████████| 625/625 [00:50<00:00, 12.42it/s]\n",
            "100%|██████████| 625/625 [00:50<00:00, 12.32it/s]\n",
            "100%|██████████| 625/625 [00:50<00:00, 12.37it/s]\n",
            "100%|██████████| 625/625 [00:50<00:00, 12.35it/s]\n",
            "100%|██████████| 171/171 [00:13<00:00, 12.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 0.2475\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:49<00:00, 12.53it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.56it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.56it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.64it/s]\n",
            "100%|██████████| 171/171 [00:13<00:00, 12.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 0.1732\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:49<00:00, 12.64it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.58it/s]\n",
            "100%|██████████| 625/625 [00:50<00:00, 12.32it/s]\n",
            "100%|██████████| 625/625 [00:50<00:00, 12.48it/s]\n",
            "100%|██████████| 171/171 [00:13<00:00, 12.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Loss: 0.1574\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:49<00:00, 12.71it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.67it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.65it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.60it/s]\n",
            "100%|██████████| 171/171 [00:13<00:00, 12.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Loss: 0.1517\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:49<00:00, 12.70it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.63it/s]\n",
            "100%|██████████| 625/625 [00:50<00:00, 12.47it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.59it/s]\n",
            "100%|██████████| 171/171 [00:13<00:00, 12.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 Loss: 0.1515\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:49<00:00, 12.65it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.64it/s]\n",
            "100%|██████████| 625/625 [00:50<00:00, 12.49it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.57it/s]\n",
            "100%|██████████| 171/171 [00:13<00:00, 12.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 Loss: 0.1482\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:49<00:00, 12.72it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.67it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.55it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.53it/s]\n",
            "100%|██████████| 171/171 [00:13<00:00, 12.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 Loss: 0.1383\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 625/625 [00:49<00:00, 12.75it/s]\n",
            "100%|██████████| 625/625 [00:50<00:00, 12.49it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.55it/s]\n",
            "100%|██████████| 625/625 [00:49<00:00, 12.53it/s]\n",
            "100%|██████████| 171/171 [00:13<00:00, 12.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 Loss: 0.1248\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Training with Memory Optimization\n",
        "scaler = torch.cuda.amp.GradScaler()  # Original syntax for current versions\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-5, weight_decay=0.01)\n",
        "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=3, eta_min=1e-6)\n",
        "\n",
        "gradient_accumulation = 4\n",
        "best_score = float('inf')\n",
        "\n",
        "for epoch in range(8):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    train_df = pd.read_csv(TRAIN_CSV).sample(frac=1.0)\n",
        "\n",
        "    for loader in chunked_loader(train_df):\n",
        "        for batch_idx, batch in enumerate(tqdm(loader)):\n",
        "            batch = {k: v.to('cuda', non_blocking=True) for k, v in batch.items()}\n",
        "\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                outputs = model(**batch)\n",
        "                loss = outputs.loss / gradient_accumulation\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (batch_idx + 1) % gradient_accumulation == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            epoch_loss += loss.item() * gradient_accumulation\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch+1} Loss: {epoch_loss/len(train_df):.4f}\")\n",
        "\n",
        "    if epoch_loss < best_score:\n",
        "        torch.save(model.state_dict(), f\"blip_epoch_{epoch+1}.pt\")\n",
        "        best_score = epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxV2QsIyKm57",
        "outputId": "4d88f4ca-2153-4106-def6-847c4e0e314d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-073c2c37d096>:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.inference_mode(), torch.cuda.amp.autocast():\n",
            "100%|██████████| 118/118 [02:58<00:00,  1.51s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 3771 images in 118 batches\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Optimized Inference with Batching\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, df, image_dir, transform):\n",
        "        self.df = df\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.image_ids = df['image_id'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.image_ids[idx]\n",
        "        img_path = os.path.join(self.image_dir, f\"{img_id}.jpg\")\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "            image = self.transform(image)\n",
        "        except:\n",
        "            image = torch.zeros(3, 384, 384)\n",
        "        return image, img_id\n",
        "\n",
        "# Batch processing setup\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_dataset = TestDataset(test_df, TEST_IMAGE_DIR, test_transform)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,  # Increased batch size\n",
        "    shuffle=False,\n",
        "    num_workers=4,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "model.load_state_dict(torch.load(\"blip_epoch_8.pt\"))\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.inference_mode(), torch.cuda.amp.autocast():\n",
        "    for images, img_ids in tqdm(test_loader):\n",
        "        images = images.to('cuda', non_blocking=True)\n",
        "        inputs = processor(images=images, return_tensors=\"pt\", do_rescale=False).to('cuda')\n",
        "\n",
        "        # Faster generation config\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=32,\n",
        "            num_beams=3,  # Reduced from 5\n",
        "            early_stopping=True,\n",
        "            temperature=0.85,\n",
        "            repetition_penalty=1.2,\n",
        "            num_return_sequences=1,\n",
        "            do_sample=False  # Faster than sampling\n",
        "        )\n",
        "\n",
        "        # Batch decoding\n",
        "        captions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
        "        predictions.extend([{\n",
        "            'image_id': img_id,\n",
        "            'caption': caption\n",
        "        } for img_id, caption in zip(img_ids, captions)])\n",
        "\n",
        "pd.DataFrame(predictions).to_csv(\"fast_submission.csv\", index=False)\n",
        "print(f\"Processed {len(predictions)} images in {len(test_loader)} batches\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAjG243YaEVN",
        "outputId": "98f6a246-3df9-43b4-feba-509ed5109081"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-27-328cd0438964>:6: UserWarning: \n",
            "⚠️ Using training captions for FGD evaluation is not valid for final metrics! This only measures how similar generated captions are to training texts, not actual test performance.\n",
            "  warnings.warn(\n",
            "<ipython-input-27-328cd0438964>:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Encoding: 100%|██████████| 15/15 [00:01<00:00,  8.55batch/s]\n",
            "Encoding: 100%|██████████| 15/15 [00:01<00:00,  9.20batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Development FGD Score (Training-based): 0.44469\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Training-based FGD Calculation (For Development Only)\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Warning about methodology\n",
        "warnings.warn(\n",
        "    \"\\n⚠️ Using training captions for FGD evaluation is not valid for final metrics! \"\n",
        "    \"This only measures how similar generated captions are to training texts, not actual test performance.\",\n",
        "    UserWarning\n",
        ")\n",
        "\n",
        "# Load data with memory optimization\n",
        "gt_df = pd.read_csv(TRAIN_CSV, dtype={'caption': 'string'}).dropna(subset=['caption'])\n",
        "sub_df = pd.read_csv(\"fast_submission.csv\", dtype={'caption': 'string'}).dropna(subset=['caption'])\n",
        "\n",
        "# Align by index (since image_ids don't match)\n",
        "N = min(len(gt_df), len(sub_df))\n",
        "gt_captions = gt_df['caption'].iloc[:N].tolist()\n",
        "pred_captions = sub_df['caption'].iloc[:N].tolist()\n",
        "\n",
        "# Initialize model on GPU\n",
        "gte_model = SentenceTransformer(\"thenlper/gte-small\", device='cuda')\n",
        "\n",
        "# Batch encoding with memory management\n",
        "def batch_encode(model, texts, batch_size=256):\n",
        "    embeddings = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\", unit=\"batch\"):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        embeddings.append(model.encode(batch, convert_to_numpy=True))\n",
        "    return np.concatenate(embeddings)\n",
        "\n",
        "# Encode with mixed precision\n",
        "with torch.cuda.amp.autocast():\n",
        "    gt_emb = batch_encode(gte_model, gt_captions)\n",
        "    pred_emb = batch_encode(gte_model, pred_captions)\n",
        "\n",
        "# Calculate FGD\n",
        "fgd_score = calculate_fgd(gt_emb, pred_emb)\n",
        "print(f\"\\nDevelopment FGD Score (Training-based): {fgd_score:.5f}\")\n",
        "\n",
        "# Cleanup\n",
        "del gte_model, gt_emb, pred_emb\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
